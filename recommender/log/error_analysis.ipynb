{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "LOG_NAME = \"recommender-model-1\"\n",
    "REMARK = \"\"\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCH = 45\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "LOSS_FN = tf.keras.losses.MeanAbsoluteError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip this: Avg Rating ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not using user data for a while"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is generated randomly using Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"../data/synt_data_with_average_rating/\")\n",
    "INFLUENCER_FILE = os.path.join(DATA_DIR, \"data_content_influencer_categ.csv\")\n",
    "OWNER_FILE = os.path.join(DATA_DIR, \"data_content_owner_categ.csv\")\n",
    "HISTORY_FILE = os.path.join(DATA_DIR, \"historical_data.csv\")\n",
    "\n",
    "df_influencer = pd.read_csv(INFLUENCER_FILE)\n",
    "# df_owner = pd.read_csv(OWNER_FILE)\n",
    "df_history = pd.read_csv(HISTORY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_influencer.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_influencer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_influencer.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_influencer.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Irrelevant Data / Invalid Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all history has valid influencer and owner ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history[\"inf_id\"].isin(df_influencer[\"id\"]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All history data has valid influencer and owner ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize influencer data: Scale follower count and One-hot categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(df, column):\n",
    "    one_hot = df[column].str.get_dummies()\n",
    "    col_name = one_hot.columns\n",
    "    new_name = list(map(lambda name: column + \"_\" + name, col_name))\n",
    "    one_hot.rename(columns={k: v for k, v in zip(col_name, new_name)}, inplace=True)\n",
    "\n",
    "    df = pd.concat([df, one_hot], axis=1)\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follower_scaler = MinMaxScaler()\n",
    "rating_scaler = MinMaxScaler()\n",
    "\n",
    "df_inf_norm = df_influencer.copy()\n",
    "df_inf_norm['avg_rating'] = rating_scaler.fit_transform(df_inf_norm[['avg_rating']])\n",
    "# df_inf_norm[[\"insta_follower\", \"tiktok\", \"youtube\"]] = follower_scaler.fit_transform(df_inf_norm[[\"insta_follower\", \"tiktok\", \"youtube\"]])\n",
    "\n",
    "df_inf_norm = one_hot(df_inf_norm, 'price_category') \n",
    "\n",
    "one_hot_categories = df_inf_norm['categories'].str.get_dummies(sep=',')\n",
    "df_inf_norm = pd.concat([df_inf_norm, one_hot_categories], axis=1)\n",
    "df_inf_norm = df_inf_norm.drop('categories', axis=1)\n",
    "\n",
    "df_inf_norm = one_hot(df_inf_norm, 'youtube') \n",
    "df_inf_norm = one_hot(df_inf_norm, 'tiktok') \n",
    "df_inf_norm = one_hot(df_inf_norm, 'insta_follower') \n",
    "\n",
    "df_inf_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine star and sentiment rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAR_WEIGHT = 0.6\n",
    "SENTIMENT_WEIGHT = 0.4\n",
    "\n",
    "df_history[\"combined_rating\"] = STAR_WEIGHT * df_history[\"star_rating\"] / 5 + SENTIMENT_WEIGHT * df_history[\"sentiment_rating\"]\n",
    "df_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_count = df_history.groupby(\"star_rating\").count()\n",
    "rating_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart labels\n",
    "rating_count = rating_count[\"own_id\"].to_numpy()\n",
    "ratings = range(1, 6)\n",
    "\n",
    "# Show pie chart\n",
    "plt.title(\"Label Distribution\")\n",
    "plt.bar(x=ratings, height=rating_count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating user profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history = df_history.drop([\"star_rating\", \"sentiment_rating\"], axis=1)\n",
    "df_inf_features = pd.merge(df_history, df_inf_norm, left_on='inf_id', right_on='id', how='left')\n",
    "df_inf_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OWNER_FEATURES = df_inf_norm.columns[1:]\n",
    "\n",
    "# Copy influencer features combined with history data\n",
    "df_own_norm = df_inf_features.copy()\n",
    "\n",
    "# Multiply influencer feature with user rating\n",
    "df_own_norm[OWNER_FEATURES] = df_own_norm[OWNER_FEATURES].mul(df_own_norm['combined_rating'], axis=0) \n",
    "\n",
    "# Drop unimportant features\n",
    "df_own_norm = df_own_norm.drop([\"inf_id\", \"id\", \"combined_rating\"], axis=1)\n",
    "\n",
    "# Average those with same owner id to make user profile\n",
    "df_own_norm = df_own_norm.groupby('own_id').mean().reset_index()\n",
    "df_own_norm.rename(columns={'own_id': 'id'}, inplace=True)\n",
    "\n",
    "df_own_norm = df_own_norm.drop(['avg_rating'], axis=1)\n",
    "df_own_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process feature and label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Influencer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ID and labels\n",
    "df_inf_features = df_inf_features.drop([\"own_id\", \"inf_id\", \"id\"], axis=1)\n",
    "\n",
    "df_inf_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inf_features_pos = df_inf_features[df_inf_features[\"combined_rating\"] > 0.6].drop([\"combined_rating\"], axis=1)\n",
    "df_inf_features_neg = df_inf_features[df_inf_features[\"combined_rating\"] <= 0.6].drop([\"combined_rating\"], axis=1)\n",
    "\n",
    "df_inf_features_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFLUENCER_FEATURE_COUNT = len(df_inf_features.drop(\"combined_rating\", axis=1).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Owner features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join history and owner data by own_id\n",
    "df_own_features = pd.merge(df_history, df_own_norm, left_on='own_id', right_on='id', how='left')\n",
    "\n",
    "df_own_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ID and labels\n",
    "df_own_features = df_own_features.drop([\"own_id\", \"inf_id\", \"id\"], axis=1)\n",
    "\n",
    "df_own_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_own_features_pos = df_own_features[df_own_features[\"combined_rating\"] > 0.6].drop([\"combined_rating\"], axis=1)\n",
    "df_own_features_neg = df_own_features[df_own_features[\"combined_rating\"] <= 0.6].drop([\"combined_rating\"], axis=1)\n",
    "\n",
    "df_own_features_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OWNER_FEATURE_COUNT = len(df_own_features.drop(\"combined_rating\", axis=1).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels from history data\n",
    "df_labels = df_history[\"combined_rating\"]\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_pos = df_labels[df_labels > 0.6]\n",
    "df_labels_neg = df_labels[df_labels <= 0.6]\n",
    "\n",
    "df_labels_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate train, validation, and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 1000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({\"inf_feature\": df_inf_features, \"own_feature\": df_own_features}, df_labels))\n",
    "# dataset = dataset.shuffle(SHUFFLE_BUFFER) \n",
    "\n",
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training, validation, and testing data\n",
    "DATASET_SIZE = dataset.cardinality().numpy()\n",
    "TRAIN_SIZE = int(DATASET_SIZE * 0.9)\n",
    "VAL_SIZE = int(DATASET_SIZE * 0.05)\n",
    "TEST_SIZE = DATASET_SIZE - TRAIN_SIZE - VAL_SIZE\n",
    "\n",
    "train_dataset = dataset.take(TRAIN_SIZE)\n",
    "val_dataset = dataset.skip(TRAIN_SIZE).take(VAL_SIZE)\n",
    "test_dataset = dataset.skip(TRAIN_SIZE + VAL_SIZE).take(TEST_SIZE)\n",
    "\n",
    "print(f\"Training dataset has {train_dataset.cardinality().numpy()} data\")\n",
    "print(f\"Validation dataset has {val_dataset.cardinality().numpy()} data\")\n",
    "print(f\"Testing dataset has {test_dataset.cardinality().numpy()} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "REPEAT = 2\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).repeat(REPEAT)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate train, validation, and test dataset (positive and negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 1000\n",
    "\n",
    "dataset_pos = tf.data.Dataset.from_tensor_slices(({\"inf_feature\": df_inf_features_pos, \"own_feature\": df_own_features_pos}, df_labels_pos))\n",
    "# dataset_pos = dataset_pos.shuffle(SHUFFLE_BUFFER) \n",
    "dataset_neg = tf.data.Dataset.from_tensor_slices(({\"inf_feature\": df_inf_features_neg, \"own_feature\": df_own_features_neg}, df_labels_neg))\n",
    "# dataset_neg = dataset_neg.shuffle(SHUFFLE_BUFFER) \n",
    "\n",
    "print(\"Positive data:\", dataset_pos.element_spec)\n",
    "print(\"Count positive data:\", dataset_pos.cardinality().numpy())\n",
    "\n",
    "print(\"\\nNegative data:\", dataset_neg.element_spec)\n",
    "print(\"Count negative data:\", dataset_neg.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training, validation, and testing data\n",
    "POSITIVE_SIZE = dataset_pos.cardinality().numpy()\n",
    "TRAIN_POS_SIZE = int(POSITIVE_SIZE * 0.9)\n",
    "VAL_POS_SIZE = int(POSITIVE_SIZE * 0.05)\n",
    "TEST_POS_SIZE = POSITIVE_SIZE - TRAIN_POS_SIZE - VAL_POS_SIZE\n",
    "\n",
    "train_dataset_pos = dataset_pos.take(TRAIN_POS_SIZE)\n",
    "val_dataset_pos = dataset_pos.skip(TRAIN_POS_SIZE).take(VAL_POS_SIZE)\n",
    "test_dataset_pos = dataset_pos.skip(TRAIN_POS_SIZE + VAL_POS_SIZE).take(TEST_POS_SIZE)\n",
    "\n",
    "print(f\"Positive training dataset has {train_dataset_pos.cardinality().numpy()} data\")\n",
    "print(f\"Positive validation dataset has {val_dataset_pos.cardinality().numpy()} data\")\n",
    "print(f\"Positive testing dataset has {test_dataset_pos.cardinality().numpy()} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training, validation, and testing data\n",
    "NEGATIVE_SIZE = dataset_neg.cardinality().numpy()\n",
    "TRAIN_NEG_SIZE = int(NEGATIVE_SIZE * 0.5)\n",
    "VAL_NEG_SIZE = int(NEGATIVE_SIZE * 0.25)\n",
    "TEST_NEG_SIZE = NEGATIVE_SIZE - TRAIN_NEG_SIZE - VAL_NEG_SIZE\n",
    "\n",
    "train_dataset_neg = dataset_neg.take(TRAIN_NEG_SIZE)\n",
    "val_dataset_neg = dataset_neg.skip(TRAIN_NEG_SIZE).take(VAL_NEG_SIZE)\n",
    "test_dataset_neg = dataset_neg.skip(TRAIN_NEG_SIZE + VAL_NEG_SIZE).take(TEST_NEG_SIZE)\n",
    "\n",
    "print(f\"Negative training dataset has {train_dataset_neg.cardinality().numpy()} data\")\n",
    "print(f\"Negative validation dataset has {val_dataset_neg.cardinality().numpy()} data\")\n",
    "print(f\"Negative testing dataset has {test_dataset_neg.cardinality().numpy()} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset_pos.concatenate(train_dataset_neg)\n",
    "val_dataset = val_dataset_pos.concatenate(val_dataset_neg)\n",
    "test_dataset = test_dataset_pos.concatenate(test_dataset_neg)\n",
    "\n",
    "print(f\"Training dataset has {train_dataset.cardinality().numpy()} data\")\n",
    "print(f\"Validation dataset has {val_dataset.cardinality().numpy()} data\")\n",
    "print(f\"Testing dataset has {test_dataset.cardinality().numpy()} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "REPEAT = 2\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MODEL = \"recommender-avgrating-moredropout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"model/summary/\", MODEL + \".txt\")) as f:\n",
    "    print(\"\".join(f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "export_path = f\"./model/savedmodel/{MODEL}/\"\n",
    "model = tf.saved_model.load(export_path)\n",
    "infer = model.signatures[\"serving_default\"]\n",
    "print(infer.inputs, \"\\n\\n\", infer.outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor(array):\n",
    "    return tf.expand_dims(tf.convert_to_tensor(array, tf.float32), 0)\n",
    "\n",
    "def convert_dataset_to_numpy(dataset):\n",
    "    inputs = []\n",
    "    labels = np.array([])\n",
    "    for batch in dataset:\n",
    "        # batch_input = []\n",
    "        # for data in batch[0]['inf_feature']:\n",
    "        #     batch_input.append({'inputs_0': data})\n",
    "        \n",
    "        # for i, data in enumerate(batch[0]['own_feature']):\n",
    "        #     batch_input[i]['inputs_1'] = data\n",
    "\n",
    "        # inputs.append(batch_input)\n",
    "        inf_feature = batch[0]['inf_feature'].numpy()\n",
    "        own_feature = batch[0]['own_feature'].numpy()\n",
    "\n",
    "        \n",
    "        # print(inf_feature, own_feature)\n",
    "        inputs += [{\"inf_feature\": convert_tensor(a_val), \"own_feature\": convert_tensor(b_val)} for a_val, b_val in zip(inf_feature, own_feature)]\n",
    "\n",
    "        labels = np.concatenate([labels, batch[1].numpy()])\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "def get_comparation_from_dataset(dataset, infer):\n",
    "    inputs, labels = convert_dataset_to_numpy(dataset)\n",
    "    \n",
    "    predict = []\n",
    "    for data in inputs:\n",
    "        predict.append(infer(**data))\n",
    "\n",
    "    predict = list(pred['dot_2'].numpy()[0] for pred in predict)\n",
    "\n",
    "    compare = pd.concat([pd.DataFrame(predict, columns=[\"predicted\"]), \n",
    "                     pd.DataFrame(labels, columns=[\"real\"])], axis=1)\n",
    "\n",
    "    compare[\"error\"] = abs(compare[\"predicted\"] - compare[\"real\"])\n",
    "\n",
    "    return compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_train = get_comparation_from_dataset(train_dataset, infer)\n",
    "# compare_val = get_comparation_from_dataset(val_dataset, infer)\n",
    "compare_test = get_comparation_from_dataset(dataset_pos.batch(BATCH_SIZE).concatenate(dataset_neg.batch(BATCH_SIZE)), infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_test['error'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get owner and influencer data\n",
    "pos_owner_inf = df_history.iloc[df_inf_features_pos.index]\n",
    "neg_owner_inf = df_history.iloc[df_inf_features_neg.index]\n",
    "combined_owner_inf = pd.concat([pos_owner_inf, neg_owner_inf], axis=0).drop('combined_rating', axis=1)\n",
    "combined_owner_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([combined_owner_inf.reset_index(), compare_test.reset_index()], axis=1)\n",
    "test = test.drop('index', axis=1)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_rank = test.groupby('own_id').apply(lambda x: x.sort_values(by='real', ascending=False)['inf_id'].tolist())\n",
    "predicted_rank = test.groupby('own_id').apply(lambda x: x.sort_values(by='predicted', ascending=False)['inf_id'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "corr_score = []\n",
    "count = []\n",
    "for id, rank in real_rank.iteritems():\n",
    "    # print(predicted_rank[id])\n",
    "    correlation, _ = kendalltau(list(rank), list(predicted_rank[id]))\n",
    "    corr_score.append(correlation)\n",
    "    count.append(len(list(rank)))\n",
    "\n",
    "df_corr = pd.concat([real_rank, predicted_rank, pd.Series(corr_score, index=real_rank.index), pd.Series(count, index=real_rank.index)], axis=1)\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.merge(test, df_influencer, how='left', left_on ='inf_id', right_on='id')\n",
    "test = pd.merge(test, df_own_norm, how='left', left_on='own_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"test.csv\")\n",
    "df_corr.to_csv(\"correlations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_val['error'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_test['error'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stopper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not using user data for a while"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is generated randomly using Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"../data/synt_data_with_average_rating/\")\n",
    "INFLUENCER_FILE = os.path.join(DATA_DIR, \"data_content_influencer_categ.csv\")\n",
    "OWNER_FILE = os.path.join(DATA_DIR, \"data_content_owner_categ.csv\")\n",
    "HISTORY_FILE = os.path.join(DATA_DIR, \"historical_data.csv\")\n",
    "\n",
    "df_influencer = pd.read_csv(INFLUENCER_FILE)\n",
    "# df_owner = pd.read_csv(OWNER_FILE)\n",
    "df_history = pd.read_csv(HISTORY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_influencer.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_influencer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_influencer.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_influencer.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Irrelevant Data / Invalid Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all history has valid influencer and owner ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history[\"inf_id\"].isin(df_influencer[\"id\"]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All history data has valid influencer and owner ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize influencer data: Scale follower count and One-hot categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follower_scaler = MinMaxScaler()\n",
    "\n",
    "df_inf_norm = df_influencer.copy()\n",
    "df_inf_norm[[\"insta_follower\", \"tiktok\", \"youtube\"]] = follower_scaler.fit_transform(df_inf_norm[[\"insta_follower\", \"tiktok\", \"youtube\"]])\n",
    "\n",
    "one_hot_price = df_inf_norm['price_category'].str.get_dummies()\n",
    "df_inf_norm = pd.concat([df_inf_norm, one_hot_price], axis=1)\n",
    "df_inf_norm = df_inf_norm.drop('price_category', axis=1)\n",
    "\n",
    "one_hot_categories = df_inf_norm['categories'].str.get_dummies(sep=',')\n",
    "df_inf_norm = pd.concat([df_inf_norm, one_hot_categories], axis=1)\n",
    "df_inf_norm = df_inf_norm.drop('categories', axis=1)\n",
    "\n",
    "df_inf_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove some category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_count = df_inf_norm[df_inf_norm.columns[8:]].sum().sort_values(ascending=False)\n",
    "\n",
    "column_name = list(categories_count.nlargest(10).index)\n",
    "column_name\n",
    "# plt.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine star and sentiment rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAR_WEIGHT = 0.6\n",
    "SENTIMENT_WEIGHT = 0.4\n",
    "\n",
    "df_history[\"combined_rating\"] = STAR_WEIGHT * df_history[\"star_rating\"] / 5 + SENTIMENT_WEIGHT * df_history[\"sentiment_rating\"]\n",
    "df_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_count = df_history.groupby(\"star_rating\").count()\n",
    "rating_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart labels\n",
    "rating_count = rating_count[\"own_id\"].to_numpy()\n",
    "ratings = range(1, 6)\n",
    "\n",
    "# Show pie chart\n",
    "plt.title(\"Label Distribution\")\n",
    "plt.bar(x=ratings, height=rating_count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating user profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history = df_history.drop([\"star_rating\", \"sentiment_rating\"], axis=1)\n",
    "df_inf_features = pd.merge(df_history, df_inf_norm, left_on='inf_id', right_on='id', how='left')\n",
    "df_inf_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OWNER_FEATURES = df_inf_norm.columns[1:]\n",
    "\n",
    "# Copy influencer features combined with history data\n",
    "df_own_norm = df_inf_features.copy()\n",
    "\n",
    "# Multiply influencer feature with user rating\n",
    "df_own_norm[OWNER_FEATURES] = df_own_norm[OWNER_FEATURES].mul(df_own_norm['combined_rating'], axis=0) \n",
    "\n",
    "# Drop unimportant features\n",
    "df_own_norm = df_own_norm.drop([\"inf_id\", \"id\", \"combined_rating\"], axis=1)\n",
    "\n",
    "# Average those with same owner id to make user profile\n",
    "df_own_norm = df_own_norm.groupby('own_id').mean().reset_index()\n",
    "df_own_norm.rename(columns={'own_id': 'id'}, inplace=True)\n",
    "\n",
    "df_own_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process feature and label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Influencer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ID and labels\n",
    "df_inf_features = df_inf_features.drop([\"own_id\", \"inf_id\", \"id\"], axis=1)\n",
    "\n",
    "df_inf_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inf_features_pos = df_inf_features[df_inf_features[\"combined_rating\"] > 0.6].drop([\"combined_rating\"], axis=1)\n",
    "df_inf_features_neg = df_inf_features[df_inf_features[\"combined_rating\"] <= 0.6].drop([\"combined_rating\"], axis=1)\n",
    "\n",
    "df_inf_features_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFLUENCER_FEATURE_COUNT = len(df_inf_features.drop(\"combined_rating\", axis=1).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Owner features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join history and owner data by own_id\n",
    "df_own_features = pd.merge(df_history, df_own_norm, left_on='own_id', right_on='id', how='left')\n",
    "\n",
    "df_own_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ID and labels\n",
    "df_own_features = df_own_features.drop([\"own_id\", \"inf_id\", \"id\"], axis=1)\n",
    "\n",
    "df_own_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_own_features_pos = df_own_features[df_own_features[\"combined_rating\"] > 0.6].drop([\"combined_rating\"], axis=1)\n",
    "df_own_features_neg = df_own_features[df_own_features[\"combined_rating\"] <= 0.6].drop([\"combined_rating\"], axis=1)\n",
    "\n",
    "df_own_features_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OWNER_FEATURE_COUNT = len(df_own_features.drop(\"combined_rating\", axis=1).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels from history data\n",
    "df_labels = df_history[\"combined_rating\"]\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_pos = df_labels[df_labels > 0.6]\n",
    "df_labels_neg = df_labels[df_labels <= 0.6]\n",
    "\n",
    "df_labels_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate train, validation, and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 1000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({\"inf_feature\": df_inf_features, \"own_feature\": df_own_features}, df_labels))\n",
    "dataset = dataset.shuffle(SHUFFLE_BUFFER) \n",
    "\n",
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training, validation, and testing data\n",
    "DATASET_SIZE = dataset.cardinality().numpy()\n",
    "TRAIN_SIZE = int(DATASET_SIZE * 0.9)\n",
    "VAL_SIZE = int(DATASET_SIZE * 0.05)\n",
    "TEST_SIZE = DATASET_SIZE - TRAIN_SIZE - VAL_SIZE\n",
    "\n",
    "train_dataset = dataset.take(TRAIN_SIZE)\n",
    "val_dataset = dataset.skip(TRAIN_SIZE).take(VAL_SIZE)\n",
    "test_dataset = dataset.skip(TRAIN_SIZE + VAL_SIZE).take(TEST_SIZE)\n",
    "\n",
    "print(f\"Training dataset has {train_dataset.cardinality().numpy()} data\")\n",
    "print(f\"Validation dataset has {val_dataset.cardinality().numpy()} data\")\n",
    "print(f\"Testing dataset has {test_dataset.cardinality().numpy()} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "REPEAT = 2\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).repeat(REPEAT)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate train, validation, and test dataset (positive and negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 1000\n",
    "\n",
    "dataset_pos = tf.data.Dataset.from_tensor_slices(({\"inf_feature\": df_inf_features_pos, \"own_feature\": df_own_features_pos}, df_labels_pos))\n",
    "# dataset_pos = dataset_pos.shuffle(SHUFFLE_BUFFER) \n",
    "dataset_neg = tf.data.Dataset.from_tensor_slices(({\"inf_feature\": df_inf_features_neg, \"own_feature\": df_own_features_neg}, df_labels_neg))\n",
    "# dataset_neg = dataset_neg.shuffle(SHUFFLE_BUFFER) \n",
    "\n",
    "print(\"Positive data:\", dataset_pos.element_spec)\n",
    "print(\"Count positive data:\", dataset_pos.cardinality().numpy())\n",
    "\n",
    "print(\"\\nNegative data:\", dataset_neg.element_spec)\n",
    "print(\"Count negative data:\", dataset_neg.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training, validation, and testing data\n",
    "POSITIVE_SIZE = dataset_pos.cardinality().numpy()\n",
    "TRAIN_POS_SIZE = int(POSITIVE_SIZE * 0.9)\n",
    "VAL_POS_SIZE = int(POSITIVE_SIZE * 0.05)\n",
    "TEST_POS_SIZE = POSITIVE_SIZE - TRAIN_POS_SIZE - VAL_POS_SIZE\n",
    "\n",
    "train_dataset_pos = dataset_pos.take(TRAIN_POS_SIZE)\n",
    "val_dataset_pos = dataset_pos.skip(TRAIN_POS_SIZE).take(VAL_POS_SIZE)\n",
    "test_dataset_pos = dataset_pos.skip(TRAIN_POS_SIZE + VAL_POS_SIZE).take(TEST_POS_SIZE)\n",
    "\n",
    "print(f\"Positive training dataset has {train_dataset_pos.cardinality().numpy()} data\")\n",
    "print(f\"Positive validation dataset has {val_dataset_pos.cardinality().numpy()} data\")\n",
    "print(f\"Positive testing dataset has {test_dataset_pos.cardinality().numpy()} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training, validation, and testing data\n",
    "NEGATIVE_SIZE = dataset_neg.cardinality().numpy()\n",
    "TRAIN_NEG_SIZE = int(NEGATIVE_SIZE * 0.5)\n",
    "VAL_NEG_SIZE = int(NEGATIVE_SIZE * 0.25)\n",
    "TEST_NEG_SIZE = NEGATIVE_SIZE - TRAIN_NEG_SIZE - VAL_NEG_SIZE\n",
    "\n",
    "train_dataset_neg = dataset_neg.take(TRAIN_NEG_SIZE)\n",
    "val_dataset_neg = dataset_neg.skip(TRAIN_NEG_SIZE).take(VAL_NEG_SIZE)\n",
    "test_dataset_neg = dataset_neg.skip(TRAIN_NEG_SIZE + VAL_NEG_SIZE).take(TEST_NEG_SIZE)\n",
    "\n",
    "print(f\"Negative training dataset has {train_dataset_neg.cardinality().numpy()} data\")\n",
    "print(f\"Negative validation dataset has {val_dataset_neg.cardinality().numpy()} data\")\n",
    "print(f\"Negative testing dataset has {test_dataset_neg.cardinality().numpy()} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset_pos.concatenate(train_dataset_neg)\n",
    "val_dataset = val_dataset_pos.concatenate(val_dataset_neg)\n",
    "test_dataset = test_dataset_pos.concatenate(test_dataset_neg)\n",
    "\n",
    "print(f\"Training dataset has {train_dataset.cardinality().numpy()} data\")\n",
    "print(f\"Validation dataset has {val_dataset.cardinality().numpy()} data\")\n",
    "print(f\"Testing dataset has {test_dataset.cardinality().numpy()} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "REPEAT = 2\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4735e28f7f627569bc07f7dc41131ed5d08a93f3d94f12e74778d501b659619"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
